{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39adcb75",
   "metadata": {},
   "source": [
    "# Improved ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250377e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, explained_variance_score\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils import load_it_data, visualize_img\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gdown\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error,explained_variance_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from utils import load_it_data,visualize_img\n",
    "\n",
    "path_to_data = '' ## Insert the folder where the data is, if you download in the same folder as this notebook then leave it blank\n",
    "\n",
    "# stim: picture / object: object name + idx / spikes: rate of each spike per stim\n",
    "\n",
    "stimulus_train, stimulus_val, stimulus_test, objects_train, objects_val, objects_test, spikes_train, spikes_val = load_it_data(path_to_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17108c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "Output shape of layer3:\t torch.Size([2592, 200704])\n",
      "\n",
      "VALIDATION\n",
      "Output shape of layer3:\t torch.Size([288, 200704])\n"
     ]
    }
   ],
   "source": [
    "seed = 35\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "stimulus_train = torch.from_numpy(stimulus_train)\n",
    "stimulus_val = torch.from_numpy(stimulus_val)\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "hooked_layer_names = ['layer3']\n",
    "\n",
    "display(device)\n",
    "\n",
    "resnet50 = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V1).to(device)\n",
    "resnet50_random = models.resnet50(weights=None).to(device)\n",
    "    \n",
    "resnet50.eval() # put in inference mode. e.g. disable dropout and batch normaliation\n",
    "resnet50_random.eval();\n",
    "\n",
    "# PRE_TRAINED RESNET\n",
    "\n",
    "module_to_layername = {id(module): name for name, module in resnet50.named_modules()}\n",
    "train_pca_models = {}\n",
    "\n",
    "def record_hook(module, input, output:torch.Tensor): # called once per entire batch\n",
    "    output_flattened = output.view(output.shape[0],-1).detach().to('cpu')\n",
    "    layer_name = module_to_layername[id(module)]\n",
    "    if train_mode:\n",
    "        pca_model = PCA(n_components=0.7).fit(output_flattened)\n",
    "        train_pca_models[layer_name] = pca_model\n",
    "    else:\n",
    "        pca_model = train_pca_models[layer_name]\n",
    "    activations_pca = pca_model.transform(output_flattened)\n",
    "    activations[layer_name] = activations_pca\n",
    "\n",
    "def shape_hook(module, input, output:torch.Tensor): \n",
    "    output_flattened = output.view(output.shape[0],-1)\n",
    "    layer_name = module_to_layername[id(module)]\n",
    "    print(f\"Output shape of {layer_name}:\\t {output_flattened.shape}\")\n",
    "\n",
    "for layer_name in hooked_layer_names:\n",
    "    layer = dict(resnet50.named_children())[layer_name]\n",
    "    layer.register_forward_hook(record_hook)\n",
    "    layer.register_forward_hook(shape_hook)\n",
    "\n",
    "# TRAIN\n",
    "train_mode = True # to guide the hook for pre-fit PCA or not\n",
    "print('TRAIN')\n",
    "activations = {}\n",
    "with torch.no_grad(): # don't compute gradiants for speeding things up\n",
    "    resnet50(stimulus_train.to(device))\n",
    "\n",
    "pca_activations_train = list(activations.values())\n",
    "# np.savez('pca_activations_train.npz',conv1=pca_activations_train[0],layer1=pca_activations_train[1],layer2=pca_activations_train[2],layer3=pca_activations_train[3],layer4=pca_activations_train[4],avg_pool=pca_activations_train[5])\n",
    "# np.savez('pca_activations_train_80p.npz', layer3=pca_activations_train[0])\n",
    "np.savez('pca_activations_train_80p.npz', layer3=pca_activations_train[0])\n",
    "\n",
    "# VALIDATION\n",
    "train_mode = False # use prefit PCA in train\n",
    "print('\\nVALIDATION')\n",
    "activations = {}\n",
    "with torch.no_grad(): # don't compute gradiants for speeding things up\n",
    "    resnet50(stimulus_val.to(device))\n",
    "\n",
    "pca_activations_val = list(activations.values())\n",
    "# np.savez('pca_activations_val_80p.npz',conv1=pca_activations_val[0],layer1=pca_activations_val[1],layer2=pca_activations_val[2],layer3=pca_activations_val[3],layer4=pca_activations_val[4],avg_pool=pca_activations_val[5])\n",
    "np.savez('pca_activations_val_80p.npz', layer3=pca_activations_val[0])\n",
    "\n",
    "# save PCA model\n",
    "np.savez('pca_model_80p.npz',layer3=train_pca_models['layer3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b7fb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: train MSE=0.13722 | val MSE=0.14053 | LR=9.30000e-05\n",
      "Epoch 002: train MSE=0.13465 | val MSE=0.13856 | LR=9.30000e-05\n",
      "Epoch 003: train MSE=0.12555 | val MSE=0.11646 | LR=9.30000e-05\n",
      "Epoch 004: train MSE=0.09541 | val MSE=0.10478 | LR=9.30000e-05\n",
      "Epoch 005: train MSE=0.08262 | val MSE=0.09796 | LR=9.30000e-05\n",
      "Epoch 006: train MSE=0.06967 | val MSE=0.09234 | LR=9.30000e-05\n",
      "Epoch 007: train MSE=0.06070 | val MSE=0.08983 | LR=9.30000e-05\n",
      "Epoch 008: train MSE=0.05370 | val MSE=0.08733 | LR=9.30000e-05\n",
      "Epoch 009: train MSE=0.04858 | val MSE=0.08537 | LR=9.30000e-05\n",
      "Epoch 010: train MSE=0.04447 | val MSE=0.08399 | LR=9.30000e-05\n",
      "Epoch 011: train MSE=0.04137 | val MSE=0.08331 | LR=9.30000e-05\n",
      "Epoch 012: train MSE=0.03856 | val MSE=0.08279 | LR=9.30000e-05\n",
      "Epoch 013: train MSE=0.03628 | val MSE=0.08216 | LR=9.30000e-05\n",
      "Epoch 014: train MSE=0.03447 | val MSE=0.08230 | LR=9.30000e-05\n",
      "Epoch 015: train MSE=0.03301 | val MSE=0.08108 | LR=9.30000e-05\n",
      "Epoch 016: train MSE=0.03167 | val MSE=0.08095 | LR=9.30000e-05\n",
      "Epoch 017: train MSE=0.03048 | val MSE=0.08124 | LR=9.30000e-05\n",
      "Epoch 018: train MSE=0.02958 | val MSE=0.08093 | LR=9.30000e-05\n",
      "Epoch 019: train MSE=0.02885 | val MSE=0.08061 | LR=9.30000e-05\n",
      "Epoch 020: train MSE=0.02783 | val MSE=0.08018 | LR=9.30000e-05\n",
      "Epoch 021: train MSE=0.02706 | val MSE=0.08063 | LR=9.30000e-05\n",
      "Epoch 022: train MSE=0.02672 | val MSE=0.07959 | LR=9.30000e-05\n",
      "Epoch 023: train MSE=0.02621 | val MSE=0.07968 | LR=9.30000e-05\n",
      "Epoch 024: train MSE=0.02567 | val MSE=0.07958 | LR=9.30000e-05\n",
      "Epoch 025: train MSE=0.02521 | val MSE=0.07966 | LR=9.30000e-05\n",
      "Epoch 026: train MSE=0.02483 | val MSE=0.07936 | LR=9.30000e-05\n",
      "Epoch 027: train MSE=0.02444 | val MSE=0.07940 | LR=9.30000e-05\n",
      "Epoch 028: train MSE=0.02411 | val MSE=0.07934 | LR=9.30000e-05\n",
      "Epoch 029: train MSE=0.02394 | val MSE=0.07922 | LR=9.30000e-05\n",
      "Epoch 030: train MSE=0.02364 | val MSE=0.07870 | LR=9.30000e-05\n",
      "Epoch 031: train MSE=0.02341 | val MSE=0.07868 | LR=9.30000e-05\n",
      "Epoch 032: train MSE=0.02312 | val MSE=0.07792 | LR=9.30000e-05\n",
      "Epoch 033: train MSE=0.02287 | val MSE=0.07748 | LR=9.30000e-05\n",
      "Epoch 034: train MSE=0.02256 | val MSE=0.07836 | LR=9.30000e-05\n",
      "Epoch 035: train MSE=0.02240 | val MSE=0.07757 | LR=9.30000e-05\n",
      "Epoch 036: train MSE=0.02235 | val MSE=0.07801 | LR=9.30000e-05\n",
      "Epoch 037: train MSE=0.02215 | val MSE=0.07732 | LR=9.30000e-05\n",
      "Epoch 038: train MSE=0.02199 | val MSE=0.07791 | LR=9.30000e-05\n",
      "Epoch 039: train MSE=0.02188 | val MSE=0.07693 | LR=9.30000e-05\n",
      "Epoch 040: train MSE=0.02173 | val MSE=0.07696 | LR=9.30000e-05\n",
      "Epoch 041: train MSE=0.02155 | val MSE=0.07658 | LR=9.30000e-05\n",
      "Epoch 042: train MSE=0.02145 | val MSE=0.07749 | LR=9.30000e-05\n",
      "Epoch 043: train MSE=0.02130 | val MSE=0.07620 | LR=9.30000e-05\n",
      "Epoch 044: train MSE=0.02122 | val MSE=0.07669 | LR=9.30000e-05\n",
      "Epoch 045: train MSE=0.02117 | val MSE=0.07611 | LR=9.30000e-05\n",
      "Epoch 046: train MSE=0.02100 | val MSE=0.07576 | LR=9.30000e-05\n",
      "Epoch 047: train MSE=0.02091 | val MSE=0.07631 | LR=9.30000e-05\n",
      "Epoch 048: train MSE=0.02094 | val MSE=0.07597 | LR=9.30000e-05\n",
      "Epoch 049: train MSE=0.02080 | val MSE=0.07602 | LR=9.30000e-05\n",
      "Epoch 050: train MSE=0.02060 | val MSE=0.07607 | LR=9.30000e-05\n",
      "Epoch 051: train MSE=0.02057 | val MSE=0.07518 | LR=9.30000e-05\n",
      "Epoch 052: train MSE=0.02066 | val MSE=0.07529 | LR=9.30000e-05\n",
      "Epoch 053: train MSE=0.02058 | val MSE=0.07548 | LR=9.30000e-05\n",
      "Epoch 054: train MSE=0.02037 | val MSE=0.07466 | LR=9.30000e-05\n",
      "Epoch 055: train MSE=0.02028 | val MSE=0.07509 | LR=9.30000e-05\n",
      "Epoch 056: train MSE=0.02029 | val MSE=0.07456 | LR=9.30000e-05\n",
      "Epoch 057: train MSE=0.02024 | val MSE=0.07457 | LR=9.30000e-05\n",
      "Epoch 058: train MSE=0.02018 | val MSE=0.07478 | LR=9.30000e-05\n",
      "Epoch 059: train MSE=0.02002 | val MSE=0.07454 | LR=9.30000e-05\n",
      "Epoch 060: train MSE=0.01996 | val MSE=0.07443 | LR=9.30000e-05\n",
      "Epoch 061: train MSE=0.01991 | val MSE=0.07477 | LR=9.30000e-05\n",
      "Epoch 062: train MSE=0.01997 | val MSE=0.07409 | LR=9.30000e-05\n",
      "Epoch 063: train MSE=0.01983 | val MSE=0.07427 | LR=9.30000e-05\n",
      "Epoch 064: train MSE=0.01986 | val MSE=0.07446 | LR=9.30000e-05\n",
      "Epoch 065: train MSE=0.01974 | val MSE=0.07427 | LR=9.30000e-05\n",
      "Epoch 066: train MSE=0.01978 | val MSE=0.07380 | LR=9.30000e-05\n",
      "Epoch 067: train MSE=0.01957 | val MSE=0.07415 | LR=9.30000e-05\n",
      "Epoch 068: train MSE=0.01952 | val MSE=0.07418 | LR=9.30000e-05\n",
      "Epoch 069: train MSE=0.01955 | val MSE=0.07361 | LR=9.30000e-05\n",
      "Epoch 070: train MSE=0.01960 | val MSE=0.07361 | LR=9.30000e-05\n",
      "Epoch 071: train MSE=0.01943 | val MSE=0.07452 | LR=9.30000e-05\n",
      "Epoch 072: train MSE=0.01939 | val MSE=0.07369 | LR=9.30000e-05\n",
      "Epoch 073: train MSE=0.01925 | val MSE=0.07393 | LR=9.30000e-05\n",
      "Epoch 074: train MSE=0.01938 | val MSE=0.07399 | LR=9.30000e-05\n",
      "Epoch 075: train MSE=0.01937 | val MSE=0.07358 | LR=9.30000e-05\n",
      "Epoch 076: train MSE=0.01935 | val MSE=0.07345 | LR=9.30000e-05\n",
      "Epoch 077: train MSE=0.01921 | val MSE=0.07381 | LR=9.30000e-05\n",
      "Epoch 078: train MSE=0.01917 | val MSE=0.07333 | LR=9.30000e-05\n",
      "Epoch 079: train MSE=0.01918 | val MSE=0.07413 | LR=9.30000e-05\n",
      "Epoch 080: train MSE=0.01911 | val MSE=0.07357 | LR=9.30000e-05\n",
      "Epoch 081: train MSE=0.01893 | val MSE=0.07270 | LR=9.30000e-05\n",
      "Epoch 082: train MSE=0.01901 | val MSE=0.07317 | LR=9.30000e-05\n",
      "Epoch 083: train MSE=0.01901 | val MSE=0.07304 | LR=9.30000e-05\n",
      "Epoch 084: train MSE=0.01908 | val MSE=0.07355 | LR=9.30000e-05\n",
      "Epoch 085: train MSE=0.01898 | val MSE=0.07359 | LR=9.30000e-05\n",
      "Epoch 086: train MSE=0.01896 | val MSE=0.07338 | LR=9.30000e-05\n",
      "Epoch 087: train MSE=0.01896 | val MSE=0.07343 | LR=9.30000e-05\n",
      "Epoch 088: train MSE=0.01879 | val MSE=0.07344 | LR=9.30000e-05\n",
      "Epoch 089: train MSE=0.01859 | val MSE=0.07306 | LR=9.30000e-05\n",
      "Epoch 090: train MSE=0.01872 | val MSE=0.07342 | LR=9.30000e-05\n",
      "Epoch 091: train MSE=0.01872 | val MSE=0.07339 | LR=9.30000e-05\n",
      "Epoch 092: train MSE=0.01863 | val MSE=0.07330 | LR=9.30000e-05\n",
      "Epoch 093: train MSE=0.01867 | val MSE=0.07297 | LR=9.30000e-05\n",
      "Epoch 094: train MSE=0.01863 | val MSE=0.07310 | LR=9.30000e-05\n",
      "Epoch 095: train MSE=0.01853 | val MSE=0.07372 | LR=9.30000e-05\n",
      "Epoch 096: train MSE=0.01846 | val MSE=0.07337 | LR=9.30000e-05\n",
      "Epoch 097: train MSE=0.01851 | val MSE=0.07314 | LR=9.30000e-05\n",
      "Epoch 098: train MSE=0.01852 | val MSE=0.07311 | LR=9.30000e-05\n",
      "Epoch 099: train MSE=0.01848 | val MSE=0.07310 | LR=9.30000e-05\n",
      "Epoch 100: train MSE=0.01850 | val MSE=0.07301 | LR=9.30000e-05\n",
      "Epoch 101: train MSE=0.01832 | val MSE=0.07321 | LR=9.30000e-05\n",
      "Epoch 102: train MSE=0.01836 | val MSE=0.07287 | LR=9.30000e-05\n",
      "Epoch 103: train MSE=0.01840 | val MSE=0.07339 | LR=9.30000e-05\n",
      "Epoch 104: train MSE=0.01832 | val MSE=0.07323 | LR=9.30000e-05\n",
      "Epoch 105: train MSE=0.01842 | val MSE=0.07310 | LR=9.30000e-05\n",
      "Epoch 106: train MSE=0.01832 | val MSE=0.07310 | LR=9.30000e-05\n",
      "Epoch 107: train MSE=0.01835 | val MSE=0.07316 | LR=9.30000e-05\n",
      "Epoch 108: train MSE=0.01834 | val MSE=0.07276 | LR=9.30000e-05\n",
      "Epoch 109: train MSE=0.01815 | val MSE=0.07341 | LR=9.30000e-05\n",
      "Epoch 110: train MSE=0.01812 | val MSE=0.07340 | LR=9.30000e-05\n",
      "Epoch 111: train MSE=0.01812 | val MSE=0.07280 | LR=9.30000e-05\n",
      "Epoch 112: train MSE=0.01818 | val MSE=0.07295 | LR=9.30000e-05\n",
      "Epoch 113: train MSE=0.01825 | val MSE=0.07311 | LR=9.30000e-05\n",
      "Epoch 114: train MSE=0.01824 | val MSE=0.07301 | LR=9.30000e-05\n",
      "Epoch 115: train MSE=0.01806 | val MSE=0.07306 | LR=9.30000e-05\n",
      "Epoch 116: train MSE=0.01811 | val MSE=0.07283 | LR=9.30000e-05\n",
      "Epoch 117: train MSE=0.01805 | val MSE=0.07272 | LR=9.30000e-05\n",
      "Epoch 118: train MSE=0.01811 | val MSE=0.07321 | LR=9.30000e-05\n",
      "Epoch 119: train MSE=0.01806 | val MSE=0.07294 | LR=9.30000e-05\n",
      "Epoch 120: train MSE=0.01803 | val MSE=0.07264 | LR=9.30000e-05\n",
      "Epoch 121: train MSE=0.01794 | val MSE=0.07300 | LR=9.30000e-05\n",
      "Epoch 122: train MSE=0.01785 | val MSE=0.07290 | LR=9.30000e-05\n",
      "Epoch 123: train MSE=0.01782 | val MSE=0.07301 | LR=9.30000e-05\n",
      "Epoch 124: train MSE=0.01777 | val MSE=0.07283 | LR=9.30000e-05\n",
      "Epoch 125: train MSE=0.01768 | val MSE=0.07268 | LR=9.30000e-05\n",
      "Epoch 126: train MSE=0.01774 | val MSE=0.07292 | LR=9.30000e-05\n",
      "Epoch 127: train MSE=0.01769 | val MSE=0.07282 | LR=9.30000e-05\n",
      "Epoch 128: train MSE=0.01777 | val MSE=0.07313 | LR=9.30000e-05\n",
      "Epoch 129: train MSE=0.01771 | val MSE=0.07260 | LR=9.30000e-05\n",
      "Epoch 130: train MSE=0.01776 | val MSE=0.07249 | LR=9.30000e-05\n",
      "Epoch 131: train MSE=0.01773 | val MSE=0.07241 | LR=9.30000e-05\n",
      "Epoch 132: train MSE=0.01784 | val MSE=0.07235 | LR=9.30000e-05\n",
      "Epoch 133: train MSE=0.01776 | val MSE=0.07249 | LR=9.30000e-05\n",
      "Epoch 134: train MSE=0.01776 | val MSE=0.07292 | LR=9.30000e-05\n",
      "Epoch 135: train MSE=0.01769 | val MSE=0.07266 | LR=9.30000e-05\n",
      "Epoch 136: train MSE=0.01770 | val MSE=0.07282 | LR=9.30000e-05\n",
      "Epoch 137: train MSE=0.01780 | val MSE=0.07232 | LR=9.30000e-05\n",
      "Epoch 138: train MSE=0.01774 | val MSE=0.07253 | LR=9.30000e-05\n",
      "Epoch 139: train MSE=0.01765 | val MSE=0.07266 | LR=9.30000e-05\n",
      "Epoch 140: train MSE=0.01762 | val MSE=0.07225 | LR=9.30000e-05\n",
      "Epoch 141: train MSE=0.01768 | val MSE=0.07287 | LR=9.30000e-05\n",
      "Epoch 142: train MSE=0.01768 | val MSE=0.07278 | LR=9.30000e-05\n",
      "Epoch 143: train MSE=0.01755 | val MSE=0.07254 | LR=9.30000e-05\n",
      "Epoch 144: train MSE=0.01754 | val MSE=0.07319 | LR=9.30000e-05\n",
      "Epoch 145: train MSE=0.01752 | val MSE=0.07294 | LR=9.30000e-05\n",
      "Epoch 146: train MSE=0.01763 | val MSE=0.07261 | LR=9.30000e-05\n",
      "Epoch 147: train MSE=0.01769 | val MSE=0.07295 | LR=9.30000e-05\n",
      "Epoch 148: train MSE=0.01769 | val MSE=0.07332 | LR=9.30000e-05\n",
      "Epoch 149: train MSE=0.01769 | val MSE=0.07265 | LR=9.30000e-05\n",
      "Epoch 150: train MSE=0.01739 | val MSE=0.07303 | LR=9.30000e-05\n",
      "Epoch 151: train MSE=0.01741 | val MSE=0.07241 | LR=9.30000e-05\n",
      "Epoch 152: train MSE=0.01727 | val MSE=0.07268 | LR=9.30000e-05\n",
      "Epoch 153: train MSE=0.01741 | val MSE=0.07293 | LR=9.30000e-05\n",
      "Epoch 154: train MSE=0.01736 | val MSE=0.07277 | LR=9.30000e-05\n",
      "Epoch 155: train MSE=0.01734 | val MSE=0.07278 | LR=9.30000e-05\n",
      "Epoch 156: train MSE=0.01727 | val MSE=0.07260 | LR=9.30000e-05\n",
      "Epoch 157: train MSE=0.01721 | val MSE=0.07304 | LR=9.30000e-05\n",
      "Epoch 158: train MSE=0.01727 | val MSE=0.07231 | LR=9.30000e-05\n",
      "Epoch 159: train MSE=0.01721 | val MSE=0.07294 | LR=9.30000e-05\n",
      "Epoch 160: train MSE=0.01726 | val MSE=0.07301 | LR=9.30000e-05\n",
      "Epoch 161: train MSE=0.01722 | val MSE=0.07282 | LR=9.30000e-05\n",
      "Epoch 162: train MSE=0.01719 | val MSE=0.07305 | LR=9.30000e-05\n",
      "Epoch 163: train MSE=0.01722 | val MSE=0.07239 | LR=9.30000e-05\n",
      "Epoch 164: train MSE=0.01723 | val MSE=0.07240 | LR=9.30000e-05\n",
      "Epoch 165: train MSE=0.01730 | val MSE=0.07240 | LR=9.30000e-05\n",
      "Epoch 166: train MSE=0.01724 | val MSE=0.07247 | LR=9.30000e-05\n",
      "Epoch 167: train MSE=0.01719 | val MSE=0.07294 | LR=9.30000e-05\n",
      "Epoch 168: train MSE=0.01714 | val MSE=0.07266 | LR=9.30000e-05\n",
      "Epoch 169: train MSE=0.01717 | val MSE=0.07283 | LR=9.30000e-05\n",
      "Epoch 170: train MSE=0.01721 | val MSE=0.07220 | LR=9.30000e-05\n",
      "Epoch 171: train MSE=0.01716 | val MSE=0.07254 | LR=9.30000e-05\n",
      "Epoch 172: train MSE=0.01714 | val MSE=0.07260 | LR=9.30000e-05\n",
      "Epoch 173: train MSE=0.01717 | val MSE=0.07309 | LR=9.30000e-05\n",
      "Epoch 174: train MSE=0.01723 | val MSE=0.07302 | LR=9.30000e-05\n",
      "Epoch 175: train MSE=0.01709 | val MSE=0.07236 | LR=9.30000e-05\n",
      "Epoch 176: train MSE=0.01712 | val MSE=0.07234 | LR=9.30000e-05\n",
      "Epoch 177: train MSE=0.01710 | val MSE=0.07234 | LR=9.30000e-05\n",
      "Epoch 178: train MSE=0.01716 | val MSE=0.07232 | LR=9.30000e-05\n",
      "Epoch 179: train MSE=0.01709 | val MSE=0.07256 | LR=9.30000e-05\n",
      "Epoch 180: train MSE=0.01707 | val MSE=0.07295 | LR=9.30000e-05\n",
      "Epoch 181: train MSE=0.01704 | val MSE=0.07235 | LR=9.30000e-05\n",
      "Epoch 182: train MSE=0.01692 | val MSE=0.07257 | LR=9.30000e-05\n",
      "Epoch 183: train MSE=0.01687 | val MSE=0.07256 | LR=9.30000e-05\n",
      "Epoch 184: train MSE=0.01710 | val MSE=0.07318 | LR=9.30000e-05\n",
      "Epoch 185: train MSE=0.01717 | val MSE=0.07256 | LR=9.30000e-05\n",
      "Epoch 186: train MSE=0.01706 | val MSE=0.07234 | LR=9.30000e-05\n",
      "Epoch 187: train MSE=0.01693 | val MSE=0.07288 | LR=9.30000e-05\n",
      "Epoch 188: train MSE=0.01698 | val MSE=0.07256 | LR=9.30000e-05\n",
      "Epoch 189: train MSE=0.01695 | val MSE=0.07248 | LR=9.30000e-05\n",
      "Epoch 190: train MSE=0.01709 | val MSE=0.07280 | LR=9.30000e-05\n",
      "Epoch 191: train MSE=0.01700 | val MSE=0.07270 | LR=9.30000e-05\n",
      "Epoch 192: train MSE=0.01685 | val MSE=0.07266 | LR=9.30000e-05\n",
      "Epoch 193: train MSE=0.01693 | val MSE=0.07251 | LR=9.30000e-05\n",
      "Epoch 194: train MSE=0.01698 | val MSE=0.07240 | LR=9.30000e-05\n",
      "Epoch 195: train MSE=0.01695 | val MSE=0.07209 | LR=9.30000e-05\n",
      "Epoch 196: train MSE=0.01685 | val MSE=0.07292 | LR=9.30000e-05\n",
      "Epoch 197: train MSE=0.01690 | val MSE=0.07230 | LR=9.30000e-05\n",
      "Epoch 198: train MSE=0.01685 | val MSE=0.07270 | LR=9.30000e-05\n",
      "Epoch 199: train MSE=0.01681 | val MSE=0.07256 | LR=9.30000e-05\n",
      "Epoch 200: train MSE=0.01683 | val MSE=0.07266 | LR=9.30000e-05\n",
      "Epoch 201: train MSE=0.01684 | val MSE=0.07242 | LR=9.30000e-05\n",
      "Epoch 202: train MSE=0.01625 | val MSE=0.07244 | LR=5.00000e-05\n",
      "Epoch 203: train MSE=0.01544 | val MSE=0.07215 | LR=5.00000e-05\n",
      "Epoch 204: train MSE=0.01530 | val MSE=0.07210 | LR=5.00000e-05\n",
      "Epoch 205: train MSE=0.01512 | val MSE=0.07228 | LR=5.00000e-05\n",
      "Epoch 206: train MSE=0.01509 | val MSE=0.07198 | LR=5.00000e-05\n",
      "Epoch 207: train MSE=0.01503 | val MSE=0.07190 | LR=5.00000e-05\n",
      "Epoch 208: train MSE=0.01512 | val MSE=0.07234 | LR=5.00000e-05\n",
      "Epoch 209: train MSE=0.01512 | val MSE=0.07186 | LR=5.00000e-05\n",
      "Epoch 210: train MSE=0.01509 | val MSE=0.07209 | LR=5.00000e-05\n",
      "Epoch 211: train MSE=0.01511 | val MSE=0.07187 | LR=5.00000e-05\n",
      "Epoch 212: train MSE=0.01502 | val MSE=0.07194 | LR=5.00000e-05\n",
      "Epoch 213: train MSE=0.01501 | val MSE=0.07208 | LR=5.00000e-05\n",
      "Epoch 214: train MSE=0.01510 | val MSE=0.07173 | LR=5.00000e-05\n",
      "Epoch 215: train MSE=0.01504 | val MSE=0.07178 | LR=5.00000e-05\n",
      "Epoch 216: train MSE=0.01514 | val MSE=0.07191 | LR=5.00000e-05\n",
      "Epoch 217: train MSE=0.01508 | val MSE=0.07206 | LR=5.00000e-05\n",
      "Epoch 218: train MSE=0.01509 | val MSE=0.07187 | LR=5.00000e-05\n",
      "Epoch 219: train MSE=0.01502 | val MSE=0.07200 | LR=5.00000e-05\n",
      "Epoch 220: train MSE=0.01506 | val MSE=0.07191 | LR=5.00000e-05\n",
      "Epoch 221: train MSE=0.01509 | val MSE=0.07189 | LR=5.00000e-05\n",
      "Epoch 222: train MSE=0.01511 | val MSE=0.07185 | LR=5.00000e-05\n",
      "Epoch 223: train MSE=0.01500 | val MSE=0.07220 | LR=5.00000e-05\n",
      "Epoch 224: train MSE=0.01506 | val MSE=0.07177 | LR=5.00000e-05\n",
      "Epoch 225: train MSE=0.01497 | val MSE=0.07170 | LR=5.00000e-05\n",
      "Epoch 226: train MSE=0.01503 | val MSE=0.07179 | LR=5.00000e-05\n",
      "Epoch 227: train MSE=0.01505 | val MSE=0.07171 | LR=5.00000e-05\n",
      "Epoch 228: train MSE=0.01506 | val MSE=0.07191 | LR=5.00000e-05\n",
      "Epoch 229: train MSE=0.01507 | val MSE=0.07197 | LR=5.00000e-05\n",
      "Epoch 230: train MSE=0.01518 | val MSE=0.07180 | LR=5.00000e-05\n",
      "Epoch 231: train MSE=0.01508 | val MSE=0.07196 | LR=5.00000e-05\n",
      "Epoch 232: train MSE=0.01505 | val MSE=0.07180 | LR=5.00000e-05\n",
      "Epoch 233: train MSE=0.01506 | val MSE=0.07174 | LR=5.00000e-05\n",
      "Epoch 234: train MSE=0.01505 | val MSE=0.07184 | LR=5.00000e-05\n",
      "Epoch 235: train MSE=0.01508 | val MSE=0.07181 | LR=5.00000e-05\n",
      "Epoch 236: train MSE=0.01510 | val MSE=0.07174 | LR=5.00000e-05\n",
      "Epoch 237: train MSE=0.01503 | val MSE=0.07169 | LR=5.00000e-05\n",
      "Epoch 238: train MSE=0.01511 | val MSE=0.07179 | LR=5.00000e-05\n",
      "Epoch 239: train MSE=0.01512 | val MSE=0.07166 | LR=5.00000e-05\n",
      "Epoch 240: train MSE=0.01512 | val MSE=0.07169 | LR=5.00000e-05\n",
      "Epoch 241: train MSE=0.01510 | val MSE=0.07191 | LR=5.00000e-05\n",
      "Epoch 242: train MSE=0.01508 | val MSE=0.07187 | LR=5.00000e-05\n",
      "Epoch 243: train MSE=0.01514 | val MSE=0.07187 | LR=5.00000e-05\n",
      "Epoch 244: train MSE=0.01503 | val MSE=0.07180 | LR=5.00000e-05\n",
      "Epoch 245: train MSE=0.01503 | val MSE=0.07151 | LR=5.00000e-05\n",
      "Epoch 246: train MSE=0.01510 | val MSE=0.07170 | LR=5.00000e-05\n",
      "Epoch 247: train MSE=0.01504 | val MSE=0.07197 | LR=5.00000e-05\n",
      "Epoch 248: train MSE=0.01509 | val MSE=0.07184 | LR=5.00000e-05\n",
      "Epoch 249: train MSE=0.01500 | val MSE=0.07203 | LR=5.00000e-05\n",
      "Epoch 250: train MSE=0.01499 | val MSE=0.07191 | LR=5.00000e-05\n",
      "Epoch 251: train MSE=0.01507 | val MSE=0.07186 | LR=5.00000e-05\n",
      "Epoch 252: train MSE=0.01501 | val MSE=0.07180 | LR=5.00000e-05\n",
      "Epoch 253: train MSE=0.01499 | val MSE=0.07175 | LR=5.00000e-05\n",
      "Epoch 254: train MSE=0.01498 | val MSE=0.07206 | LR=5.00000e-05\n",
      "Epoch 255: train MSE=0.01500 | val MSE=0.07185 | LR=5.00000e-05\n",
      "Epoch 256: train MSE=0.01499 | val MSE=0.07182 | LR=5.00000e-05\n",
      "Epoch 257: train MSE=0.01506 | val MSE=0.07161 | LR=5.00000e-05\n",
      "Epoch 258: train MSE=0.01501 | val MSE=0.07156 | LR=5.00000e-05\n",
      "Epoch 259: train MSE=0.01498 | val MSE=0.07183 | LR=5.00000e-05\n",
      "Epoch 260: train MSE=0.01515 | val MSE=0.07191 | LR=5.00000e-05\n",
      "Epoch 261: train MSE=0.01499 | val MSE=0.07178 | LR=5.00000e-05\n",
      "Epoch 262: train MSE=0.01507 | val MSE=0.07161 | LR=5.00000e-05\n",
      "Epoch 263: train MSE=0.01504 | val MSE=0.07192 | LR=5.00000e-05\n",
      "Epoch 264: train MSE=0.01504 | val MSE=0.07217 | LR=5.00000e-05\n",
      "Epoch 265: train MSE=0.01503 | val MSE=0.07185 | LR=5.00000e-05\n",
      "Epoch 266: train MSE=0.01499 | val MSE=0.07166 | LR=5.00000e-05\n",
      "Epoch 267: train MSE=0.01500 | val MSE=0.07161 | LR=5.00000e-05\n",
      "Epoch 268: train MSE=0.01497 | val MSE=0.07181 | LR=5.00000e-05\n",
      "Epoch 269: train MSE=0.01501 | val MSE=0.07183 | LR=5.00000e-05\n",
      "Epoch 270: train MSE=0.01505 | val MSE=0.07173 | LR=5.00000e-05\n",
      "Epoch 271: train MSE=0.01501 | val MSE=0.07186 | LR=5.00000e-05\n",
      "Epoch 272: train MSE=0.01500 | val MSE=0.07189 | LR=5.00000e-05\n",
      "Epoch 273: train MSE=0.01499 | val MSE=0.07160 | LR=5.00000e-05\n",
      "Epoch 274: train MSE=0.01499 | val MSE=0.07172 | LR=5.00000e-05\n",
      "Epoch 275: train MSE=0.01494 | val MSE=0.07153 | LR=5.00000e-05\n",
      "Epoch 276: train MSE=0.01496 | val MSE=0.07175 | LR=5.00000e-05\n",
      "Epoch 277: train MSE=0.01500 | val MSE=0.07159 | LR=5.00000e-05\n",
      "Epoch 278: train MSE=0.01504 | val MSE=0.07182 | LR=5.00000e-05\n",
      "Epoch 279: train MSE=0.01494 | val MSE=0.07184 | LR=5.00000e-05\n",
      "Epoch 280: train MSE=0.01501 | val MSE=0.07177 | LR=5.00000e-05\n",
      "Epoch 281: train MSE=0.01494 | val MSE=0.07207 | LR=5.00000e-05\n",
      "Epoch 282: train MSE=0.01496 | val MSE=0.07151 | LR=5.00000e-05\n",
      "Epoch 283: train MSE=0.01496 | val MSE=0.07152 | LR=5.00000e-05\n",
      "Epoch 284: train MSE=0.01498 | val MSE=0.07172 | LR=5.00000e-05\n",
      "Epoch 285: train MSE=0.01499 | val MSE=0.07132 | LR=5.00000e-05\n",
      "Epoch 286: train MSE=0.01498 | val MSE=0.07177 | LR=5.00000e-05\n",
      "Epoch 287: train MSE=0.01497 | val MSE=0.07196 | LR=5.00000e-05\n",
      "Epoch 288: train MSE=0.01496 | val MSE=0.07188 | LR=5.00000e-05\n",
      "Epoch 289: train MSE=0.01492 | val MSE=0.07196 | LR=5.00000e-05\n",
      "Epoch 290: train MSE=0.01493 | val MSE=0.07168 | LR=5.00000e-05\n",
      "Epoch 291: train MSE=0.01495 | val MSE=0.07171 | LR=5.00000e-05\n",
      "Epoch 292: train MSE=0.01495 | val MSE=0.07154 | LR=5.00000e-05\n",
      "Epoch 293: train MSE=0.01489 | val MSE=0.07155 | LR=5.00000e-05\n",
      "Epoch 294: train MSE=0.01498 | val MSE=0.07189 | LR=5.00000e-05\n",
      "Epoch 295: train MSE=0.01492 | val MSE=0.07168 | LR=5.00000e-05\n",
      "Epoch 296: train MSE=0.01496 | val MSE=0.07161 | LR=5.00000e-05\n",
      "Epoch 297: train MSE=0.01495 | val MSE=0.07161 | LR=5.00000e-05\n",
      "Epoch 298: train MSE=0.01492 | val MSE=0.07180 | LR=5.00000e-05\n",
      "Epoch 299: train MSE=0.01494 | val MSE=0.07182 | LR=5.00000e-05\n",
      "Epoch 300: train MSE=0.01492 | val MSE=0.07180 | LR=5.00000e-05\n",
      "Epoch 301: train MSE=0.01493 | val MSE=0.07154 | LR=5.00000e-05\n",
      "Epoch 302: train MSE=0.01463 | val MSE=0.07172 | LR=1.00000e-05\n",
      "Epoch 303: train MSE=0.01435 | val MSE=0.07157 | LR=1.00000e-05\n",
      "Epoch 304: train MSE=0.01418 | val MSE=0.07154 | LR=1.00000e-05\n",
      "Epoch 305: train MSE=0.01406 | val MSE=0.07146 | LR=1.00000e-05\n",
      "Epoch 306: train MSE=0.01399 | val MSE=0.07149 | LR=1.00000e-05\n",
      "Epoch 307: train MSE=0.01388 | val MSE=0.07144 | LR=1.00000e-05\n",
      "Epoch 308: train MSE=0.01383 | val MSE=0.07138 | LR=1.00000e-05\n",
      "Epoch 309: train MSE=0.01391 | val MSE=0.07135 | LR=1.00000e-05\n",
      "Epoch 310: train MSE=0.01388 | val MSE=0.07129 | LR=1.00000e-05\n",
      "Epoch 311: train MSE=0.01386 | val MSE=0.07139 | LR=1.00000e-05\n",
      "Epoch 312: train MSE=0.01384 | val MSE=0.07142 | LR=1.00000e-05\n",
      "Epoch 313: train MSE=0.01379 | val MSE=0.07128 | LR=1.00000e-05\n",
      "Epoch 314: train MSE=0.01378 | val MSE=0.07136 | LR=1.00000e-05\n",
      "Epoch 315: train MSE=0.01377 | val MSE=0.07133 | LR=1.00000e-05\n",
      "Epoch 316: train MSE=0.01373 | val MSE=0.07129 | LR=1.00000e-05\n",
      "Epoch 317: train MSE=0.01371 | val MSE=0.07133 | LR=1.00000e-05\n",
      "Epoch 318: train MSE=0.01375 | val MSE=0.07131 | LR=1.00000e-05\n",
      "Epoch 319: train MSE=0.01373 | val MSE=0.07130 | LR=1.00000e-05\n",
      "Epoch 320: train MSE=0.01376 | val MSE=0.07132 | LR=1.00000e-05\n",
      "Epoch 321: train MSE=0.01371 | val MSE=0.07130 | LR=1.00000e-05\n",
      "Epoch 322: train MSE=0.01370 | val MSE=0.07136 | LR=1.00000e-05\n",
      "Epoch 323: train MSE=0.01370 | val MSE=0.07130 | LR=1.00000e-05\n",
      "Epoch 324: train MSE=0.01372 | val MSE=0.07129 | LR=1.00000e-05\n",
      "Epoch 325: train MSE=0.01371 | val MSE=0.07120 | LR=1.00000e-05\n",
      "Epoch 326: train MSE=0.01372 | val MSE=0.07140 | LR=1.00000e-05\n",
      "Epoch 327: train MSE=0.01374 | val MSE=0.07125 | LR=1.00000e-05\n",
      "Epoch 328: train MSE=0.01375 | val MSE=0.07122 | LR=1.00000e-05\n",
      "Epoch 329: train MSE=0.01374 | val MSE=0.07127 | LR=1.00000e-05\n",
      "Epoch 330: train MSE=0.01373 | val MSE=0.07126 | LR=1.00000e-05\n",
      "Epoch 331: train MSE=0.01371 | val MSE=0.07127 | LR=1.00000e-05\n",
      "Epoch 332: train MSE=0.01373 | val MSE=0.07125 | LR=1.00000e-05\n",
      "Epoch 333: train MSE=0.01375 | val MSE=0.07131 | LR=1.00000e-05\n",
      "Epoch 334: train MSE=0.01373 | val MSE=0.07125 | LR=1.00000e-05\n",
      "Epoch 335: train MSE=0.01370 | val MSE=0.07127 | LR=1.00000e-05\n",
      "Epoch 336: train MSE=0.01370 | val MSE=0.07126 | LR=1.00000e-05\n",
      "Epoch 337: train MSE=0.01374 | val MSE=0.07121 | LR=1.00000e-05\n",
      "Epoch 338: train MSE=0.01373 | val MSE=0.07130 | LR=1.00000e-05\n",
      "Epoch 339: train MSE=0.01371 | val MSE=0.07120 | LR=1.00000e-05\n",
      "Epoch 340: train MSE=0.01372 | val MSE=0.07126 | LR=1.00000e-05\n",
      "Epoch 341: train MSE=0.01367 | val MSE=0.07120 | LR=1.00000e-05\n",
      "Epoch 342: train MSE=0.01375 | val MSE=0.07117 | LR=1.00000e-05\n",
      "Epoch 343: train MSE=0.01372 | val MSE=0.07124 | LR=1.00000e-05\n",
      "Epoch 344: train MSE=0.01370 | val MSE=0.07132 | LR=1.00000e-05\n",
      "Epoch 345: train MSE=0.01377 | val MSE=0.07129 | LR=1.00000e-05\n",
      "Epoch 346: train MSE=0.01374 | val MSE=0.07123 | LR=1.00000e-05\n",
      "Epoch 347: train MSE=0.01369 | val MSE=0.07124 | LR=1.00000e-05\n",
      "Epoch 348: train MSE=0.01371 | val MSE=0.07126 | LR=1.00000e-05\n",
      "Epoch 349: train MSE=0.01375 | val MSE=0.07127 | LR=1.00000e-05\n",
      "Epoch 350: train MSE=0.01365 | val MSE=0.07134 | LR=1.00000e-05\n",
      "Epoch 351: train MSE=0.01364 | val MSE=0.07128 | LR=1.00000e-05\n",
      "Epoch 352: train MSE=0.01372 | val MSE=0.07121 | LR=1.00000e-05\n",
      "Epoch 353: train MSE=0.01372 | val MSE=0.07130 | LR=1.00000e-05\n",
      "Epoch 354: train MSE=0.01370 | val MSE=0.07125 | LR=1.00000e-05\n",
      "Epoch 355: train MSE=0.01369 | val MSE=0.07124 | LR=1.00000e-05\n",
      "Epoch 356: train MSE=0.01368 | val MSE=0.07124 | LR=1.00000e-05\n",
      "Epoch 357: train MSE=0.01372 | val MSE=0.07116 | LR=1.00000e-05\n",
      "Epoch 358: train MSE=0.01364 | val MSE=0.07115 | LR=1.00000e-05\n",
      "Epoch 359: train MSE=0.01368 | val MSE=0.07120 | LR=1.00000e-05\n",
      "Epoch 360: train MSE=0.01369 | val MSE=0.07121 | LR=1.00000e-05\n",
      "Epoch 361: train MSE=0.01371 | val MSE=0.07118 | LR=1.00000e-05\n",
      "Epoch 362: train MSE=0.01366 | val MSE=0.07129 | LR=1.00000e-05\n",
      "Epoch 363: train MSE=0.01370 | val MSE=0.07118 | LR=1.00000e-05\n",
      "Epoch 364: train MSE=0.01368 | val MSE=0.07121 | LR=1.00000e-05\n",
      "Epoch 365: train MSE=0.01364 | val MSE=0.07117 | LR=1.00000e-05\n",
      "Epoch 366: train MSE=0.01371 | val MSE=0.07120 | LR=1.00000e-05\n",
      "Epoch 367: train MSE=0.01369 | val MSE=0.07119 | LR=1.00000e-05\n",
      "Epoch 368: train MSE=0.01372 | val MSE=0.07117 | LR=1.00000e-05\n",
      "Epoch 369: train MSE=0.01368 | val MSE=0.07119 | LR=1.00000e-05\n",
      "Epoch 370: train MSE=0.01372 | val MSE=0.07123 | LR=1.00000e-05\n",
      "Epoch 371: train MSE=0.01374 | val MSE=0.07122 | LR=1.00000e-05\n",
      "Epoch 372: train MSE=0.01365 | val MSE=0.07122 | LR=1.00000e-05\n",
      "Epoch 373: train MSE=0.01366 | val MSE=0.07115 | LR=1.00000e-05\n",
      "Epoch 374: train MSE=0.01373 | val MSE=0.07105 | LR=1.00000e-05\n",
      "Epoch 375: train MSE=0.01374 | val MSE=0.07115 | LR=1.00000e-05\n",
      "Epoch 376: train MSE=0.01371 | val MSE=0.07103 | LR=1.00000e-05\n",
      "Epoch 377: train MSE=0.01366 | val MSE=0.07110 | LR=1.00000e-05\n",
      "Epoch 378: train MSE=0.01369 | val MSE=0.07106 | LR=1.00000e-05\n",
      "Epoch 379: train MSE=0.01369 | val MSE=0.07127 | LR=1.00000e-05\n",
      "Epoch 380: train MSE=0.01371 | val MSE=0.07121 | LR=1.00000e-05\n",
      "Epoch 381: train MSE=0.01372 | val MSE=0.07112 | LR=1.00000e-05\n",
      "Epoch 382: train MSE=0.01367 | val MSE=0.07120 | LR=1.00000e-05\n",
      "Epoch 383: train MSE=0.01367 | val MSE=0.07112 | LR=1.00000e-05\n",
      "Epoch 384: train MSE=0.01366 | val MSE=0.07121 | LR=1.00000e-05\n",
      "Epoch 385: train MSE=0.01365 | val MSE=0.07109 | LR=1.00000e-05\n",
      "Epoch 386: train MSE=0.01373 | val MSE=0.07121 | LR=1.00000e-05\n",
      "Epoch 387: train MSE=0.01367 | val MSE=0.07119 | LR=1.00000e-05\n",
      "Epoch 388: train MSE=0.01369 | val MSE=0.07121 | LR=1.00000e-05\n",
      "Epoch 389: train MSE=0.01368 | val MSE=0.07111 | LR=1.00000e-05\n",
      "Epoch 390: train MSE=0.01362 | val MSE=0.07111 | LR=1.00000e-05\n",
      "Epoch 391: train MSE=0.01371 | val MSE=0.07119 | LR=1.00000e-05\n",
      "Epoch 392: train MSE=0.01362 | val MSE=0.07111 | LR=1.00000e-05\n",
      "Epoch 393: train MSE=0.01371 | val MSE=0.07113 | LR=1.00000e-05\n",
      "Epoch 394: train MSE=0.01366 | val MSE=0.07112 | LR=1.00000e-05\n",
      "Epoch 395: train MSE=0.01372 | val MSE=0.07112 | LR=1.00000e-05\n",
      "Epoch 396: train MSE=0.01364 | val MSE=0.07105 | LR=1.00000e-05\n",
      "Epoch 397: train MSE=0.01368 | val MSE=0.07119 | LR=1.00000e-05\n",
      "Epoch 398: train MSE=0.01370 | val MSE=0.07108 | LR=1.00000e-05\n",
      "Epoch 399: train MSE=0.01375 | val MSE=0.07108 | LR=1.00000e-05\n",
      "Epoch 400: train MSE=0.01367 | val MSE=0.07108 | LR=1.00000e-05\n",
      "Epoch 401: train MSE=0.01368 | val MSE=0.07115 | LR=1.00000e-05\n",
      "Epoch 402: train MSE=0.01366 | val MSE=0.07119 | LR=5.00000e-06\n",
      "Epoch 403: train MSE=0.01358 | val MSE=0.07120 | LR=5.00000e-06\n",
      "Epoch 404: train MSE=0.01358 | val MSE=0.07115 | LR=5.00000e-06\n",
      "Epoch 405: train MSE=0.01357 | val MSE=0.07119 | LR=5.00000e-06\n",
      "Epoch 406: train MSE=0.01363 | val MSE=0.07117 | LR=5.00000e-06\n",
      "Epoch 407: train MSE=0.01360 | val MSE=0.07117 | LR=5.00000e-06\n",
      "Epoch 408: train MSE=0.01361 | val MSE=0.07119 | LR=5.00000e-06\n",
      "Epoch 409: train MSE=0.01361 | val MSE=0.07117 | LR=5.00000e-06\n",
      "Epoch 410: train MSE=0.01354 | val MSE=0.07117 | LR=5.00000e-06\n",
      "Epoch 411: train MSE=0.01360 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 412: train MSE=0.01362 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 413: train MSE=0.01353 | val MSE=0.07114 | LR=5.00000e-06\n",
      "Epoch 414: train MSE=0.01362 | val MSE=0.07108 | LR=5.00000e-06\n",
      "Epoch 415: train MSE=0.01356 | val MSE=0.07115 | LR=5.00000e-06\n",
      "Epoch 416: train MSE=0.01361 | val MSE=0.07105 | LR=5.00000e-06\n",
      "Epoch 417: train MSE=0.01356 | val MSE=0.07115 | LR=5.00000e-06\n",
      "Epoch 418: train MSE=0.01357 | val MSE=0.07113 | LR=5.00000e-06\n",
      "Epoch 419: train MSE=0.01360 | val MSE=0.07111 | LR=5.00000e-06\n",
      "Epoch 420: train MSE=0.01353 | val MSE=0.07110 | LR=5.00000e-06\n",
      "Epoch 421: train MSE=0.01360 | val MSE=0.07115 | LR=5.00000e-06\n",
      "Epoch 422: train MSE=0.01353 | val MSE=0.07116 | LR=5.00000e-06\n",
      "Epoch 423: train MSE=0.01359 | val MSE=0.07114 | LR=5.00000e-06\n",
      "Epoch 424: train MSE=0.01346 | val MSE=0.07115 | LR=5.00000e-06\n",
      "Epoch 425: train MSE=0.01355 | val MSE=0.07118 | LR=5.00000e-06\n",
      "Epoch 426: train MSE=0.01358 | val MSE=0.07111 | LR=5.00000e-06\n",
      "Epoch 427: train MSE=0.01351 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 428: train MSE=0.01355 | val MSE=0.07115 | LR=5.00000e-06\n",
      "Epoch 429: train MSE=0.01357 | val MSE=0.07111 | LR=5.00000e-06\n",
      "Epoch 430: train MSE=0.01357 | val MSE=0.07110 | LR=5.00000e-06\n",
      "Epoch 431: train MSE=0.01356 | val MSE=0.07113 | LR=5.00000e-06\n",
      "Epoch 432: train MSE=0.01353 | val MSE=0.07112 | LR=5.00000e-06\n",
      "Epoch 433: train MSE=0.01353 | val MSE=0.07112 | LR=5.00000e-06\n",
      "Epoch 434: train MSE=0.01354 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 435: train MSE=0.01356 | val MSE=0.07116 | LR=5.00000e-06\n",
      "Epoch 436: train MSE=0.01353 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 437: train MSE=0.01357 | val MSE=0.07108 | LR=5.00000e-06\n",
      "Epoch 438: train MSE=0.01354 | val MSE=0.07114 | LR=5.00000e-06\n",
      "Epoch 439: train MSE=0.01357 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 440: train MSE=0.01357 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 441: train MSE=0.01352 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 442: train MSE=0.01354 | val MSE=0.07111 | LR=5.00000e-06\n",
      "Epoch 443: train MSE=0.01357 | val MSE=0.07116 | LR=5.00000e-06\n",
      "Epoch 444: train MSE=0.01353 | val MSE=0.07108 | LR=5.00000e-06\n",
      "Epoch 445: train MSE=0.01354 | val MSE=0.07116 | LR=5.00000e-06\n",
      "Epoch 446: train MSE=0.01356 | val MSE=0.07112 | LR=5.00000e-06\n",
      "Epoch 447: train MSE=0.01355 | val MSE=0.07106 | LR=5.00000e-06\n",
      "Epoch 448: train MSE=0.01360 | val MSE=0.07106 | LR=5.00000e-06\n",
      "Epoch 449: train MSE=0.01352 | val MSE=0.07113 | LR=5.00000e-06\n",
      "Epoch 450: train MSE=0.01356 | val MSE=0.07106 | LR=5.00000e-06\n",
      "Epoch 451: train MSE=0.01357 | val MSE=0.07105 | LR=5.00000e-06\n",
      "Epoch 452: train MSE=0.01357 | val MSE=0.07107 | LR=5.00000e-06\n",
      "Epoch 453: train MSE=0.01354 | val MSE=0.07104 | LR=5.00000e-06\n",
      "Epoch 454: train MSE=0.01352 | val MSE=0.07102 | LR=5.00000e-06\n",
      "Epoch 455: train MSE=0.01352 | val MSE=0.07103 | LR=5.00000e-06\n",
      "Epoch 456: train MSE=0.01355 | val MSE=0.07105 | LR=5.00000e-06\n",
      "Epoch 457: train MSE=0.01353 | val MSE=0.07102 | LR=5.00000e-06\n",
      "Epoch 458: train MSE=0.01355 | val MSE=0.07103 | LR=5.00000e-06\n",
      "Epoch 459: train MSE=0.01359 | val MSE=0.07103 | LR=5.00000e-06\n",
      "Epoch 460: train MSE=0.01352 | val MSE=0.07100 | LR=5.00000e-06\n",
      "Epoch 461: train MSE=0.01355 | val MSE=0.07104 | LR=5.00000e-06\n",
      "Epoch 462: train MSE=0.01359 | val MSE=0.07101 | LR=5.00000e-06\n",
      "Epoch 463: train MSE=0.01354 | val MSE=0.07108 | LR=5.00000e-06\n",
      "Epoch 464: train MSE=0.01356 | val MSE=0.07108 | LR=5.00000e-06\n",
      "Epoch 465: train MSE=0.01355 | val MSE=0.07104 | LR=5.00000e-06\n",
      "Epoch 466: train MSE=0.01353 | val MSE=0.07104 | LR=5.00000e-06\n",
      "Epoch 467: train MSE=0.01351 | val MSE=0.07112 | LR=5.00000e-06\n",
      "Epoch 468: train MSE=0.01355 | val MSE=0.07110 | LR=5.00000e-06\n",
      "Epoch 469: train MSE=0.01352 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 470: train MSE=0.01352 | val MSE=0.07108 | LR=5.00000e-06\n",
      "Epoch 471: train MSE=0.01349 | val MSE=0.07104 | LR=5.00000e-06\n",
      "Epoch 472: train MSE=0.01353 | val MSE=0.07101 | LR=5.00000e-06\n",
      "Epoch 473: train MSE=0.01353 | val MSE=0.07105 | LR=5.00000e-06\n",
      "Epoch 474: train MSE=0.01355 | val MSE=0.07106 | LR=5.00000e-06\n",
      "Epoch 475: train MSE=0.01355 | val MSE=0.07098 | LR=5.00000e-06\n",
      "Epoch 476: train MSE=0.01353 | val MSE=0.07104 | LR=5.00000e-06\n",
      "Epoch 477: train MSE=0.01352 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 478: train MSE=0.01357 | val MSE=0.07102 | LR=5.00000e-06\n",
      "Epoch 479: train MSE=0.01359 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 480: train MSE=0.01355 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 481: train MSE=0.01353 | val MSE=0.07104 | LR=5.00000e-06\n",
      "Epoch 482: train MSE=0.01357 | val MSE=0.07105 | LR=5.00000e-06\n",
      "Epoch 483: train MSE=0.01354 | val MSE=0.07106 | LR=5.00000e-06\n",
      "Epoch 484: train MSE=0.01353 | val MSE=0.07102 | LR=5.00000e-06\n",
      "Epoch 485: train MSE=0.01358 | val MSE=0.07105 | LR=5.00000e-06\n",
      "Epoch 486: train MSE=0.01349 | val MSE=0.07108 | LR=5.00000e-06\n",
      "Epoch 487: train MSE=0.01351 | val MSE=0.07104 | LR=5.00000e-06\n",
      "Epoch 488: train MSE=0.01348 | val MSE=0.07103 | LR=5.00000e-06\n",
      "Epoch 489: train MSE=0.01352 | val MSE=0.07109 | LR=5.00000e-06\n",
      "Epoch 490: train MSE=0.01353 | val MSE=0.07104 | LR=5.00000e-06\n",
      "Epoch 491: train MSE=0.01350 | val MSE=0.07106 | LR=5.00000e-06\n",
      "Epoch 492: train MSE=0.01352 | val MSE=0.07104 | LR=5.00000e-06\n",
      "Epoch 493: train MSE=0.01354 | val MSE=0.07107 | LR=5.00000e-06\n",
      "Epoch 494: train MSE=0.01350 | val MSE=0.07104 | LR=5.00000e-06\n",
      "Epoch 495: train MSE=0.01349 | val MSE=0.07100 | LR=5.00000e-06\n",
      "Epoch 496: train MSE=0.01355 | val MSE=0.07099 | LR=5.00000e-06\n",
      "Epoch 497: train MSE=0.01358 | val MSE=0.07100 | LR=5.00000e-06\n",
      "Epoch 498: train MSE=0.01352 | val MSE=0.07101 | LR=5.00000e-06\n",
      "Epoch 499: train MSE=0.01350 | val MSE=0.07097 | LR=5.00000e-06\n",
      "Epoch 500: train MSE=0.01352 | val MSE=0.07100 | LR=5.00000e-06\n",
      "\n",
      "=== Validation metrics ===\n",
      "Pearson r (all spikes flattened): 0.7081\n",
      "Explained variance (mean over neurons): 0.4281\n",
      "MSE (mean over neurons): 0.0710\n"
     ]
    }
   ],
   "source": [
    "seed = 35\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# # 1000 PCA components from layer3 only load\n",
    "# pca_activations_train = np.load('pca_activations_train.npz')\n",
    "# pca_activations_val   = np.load('pca_activations_val.npz')\n",
    "\n",
    "# X_train = pca_activations_train['layer3']\n",
    "# X_val   = pca_activations_val['layer3']\n",
    "# y_train = spikes_train\n",
    "# y_val   = spikes_val\n",
    "\n",
    "# 80% PCA from layer3 only load\n",
    "pca_activations_train = np.load('pca_activations_train_80p.npz')\n",
    "pca_activations_val   = np.load('pca_activations_val_80p.npz')\n",
    "\n",
    "X_train = pca_activations_train['layer3']\n",
    "X_val   = pca_activations_val['layer3']\n",
    "y_train = spikes_train\n",
    "y_val   = spikes_val\n",
    "\n",
    "# # 95% PCA from layer3 only load\n",
    "# pca_activations_train = np.load('pca_activations_train_95p.npz')\n",
    "# pca_activations_val   = np.load('pca_activations_val_95p.npz')\n",
    "\n",
    "# X_train = pca_activations_train['layer3']\n",
    "# X_val   = pca_activations_val['layer3']\n",
    "# y_train = spikes_train\n",
    "# y_val   = spikes_val\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val   = scaler.transform(X_val)\n",
    "\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val_t   = torch.tensor(X_val,   dtype=torch.float32).to(device)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.float32).to(device)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(TensorDataset(X_train_t, y_train_t),\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(X_val_t, y_val_t),\n",
    "                          batch_size=batch_size, shuffle=False)\n",
    "\n",
    "class MLPRegressor(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_dim:  int,\n",
    "                 out_dim: int,\n",
    "                 hidden=(512, 256),\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = in_dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(dropout)]\n",
    "            last = h\n",
    "        layers.append(nn.Linear(last, out_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(dim, dim)\n",
    "        self.fc2 = nn.Linear(dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        h = self.drop(F.relu(self.fc2(h)))\n",
    "        return x + h\n",
    "\n",
    "        \n",
    "def make_mlp(style: str, d_in: int, d_out: int):\n",
    "    if style == \"funnel\":\n",
    "        hidden = (4*d_in, 2*d_in, d_in)\n",
    "    elif style == \"hourglass\":\n",
    "        hidden = (d_in//2, d_in, d_in//2)\n",
    "    elif style == \"residual\":\n",
    "        first = 2*d_in\n",
    "        blocks = 3\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(d_in, first), nn.ReLU(),\n",
    "            *[ResidualBlock(first) for _ in range(blocks)],\n",
    "            nn.Linear(first, d_out)\n",
    "        )\n",
    "    layers = []\n",
    "    last = d_in\n",
    "    for h in hidden:\n",
    "        layers += [nn.Linear(last, h), nn.ReLU(), nn.Dropout(0.3)]\n",
    "        last = h\n",
    "    layers.append(nn.Linear(last, d_out))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "model = MLPRegressor(in_dim=X_train.shape[1],\n",
    "                     out_dim=y_train.shape[1]).to(device)\n",
    "\n",
    "model = make_mlp(\"funnel\", X_train.shape[1], y_train.shape[1]).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e+1, weight_decay=1e-4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=9.3e-5, weight_decay=9e-5, amsgrad=True, eps=1e-07)\n",
    "\n",
    "# training\n",
    "best_val_mse = float('inf')\n",
    "patience      = 200\n",
    "patience_ctr  = 0\n",
    "num_epochs    = 500\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    train_mse = running_loss / len(train_loader.dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            pred = model(xb)\n",
    "            val_running_loss += criterion(pred, yb).item() * xb.size(0)\n",
    "    val_mse = val_running_loss / len(val_loader.dataset)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d}: train MSE={train_mse:.5f} | val MSE={val_mse:.5f} | LR={optimizer.param_groups[0]['lr']:.5e}\")\n",
    "\n",
    "    if epoch > 200:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 5e-5\n",
    "    if epoch > 300:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 1e-5\n",
    "    if epoch > 400:\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = 5e-6\n",
    "\n",
    "    if val_mse < best_val_mse - 1e-9:\n",
    "        best_val_mse = val_mse\n",
    "        torch.save(model.state_dict(), \"best_mlp.pt\")\n",
    "        patience_ctr = 0\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "        if patience_ctr >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# eval and save\n",
    "model.load_state_dict(torch.load(\"best_mlp.pt\"))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_val = model(X_val_t).cpu().numpy()\n",
    "\n",
    "corr = pearsonr(y_val.flatten(), y_pred_val.flatten())[0]\n",
    "ev   = explained_variance_score(y_val, y_pred_val, multioutput='raw_values')\n",
    "mse  = mean_squared_error(y_val, y_pred_val, multioutput='raw_values')\n",
    "\n",
    "print(\"\\n=== Validation metrics ===\")\n",
    "print(f\"Pearson r (all spikes flattened): {corr:.4f}\")\n",
    "print(f\"Explained variance (mean over neurons): {ev.mean():.4f}\")\n",
    "print(f\"MSE (mean over neurons): {mse.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21774d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation metrics ===\n",
      "Pearson r (all spikes flattened): 0.7081\n",
      "Explained variance (mean over neurons): 0.4281\n",
      "MSE (mean over neurons): 0.0710\n",
      "Model size: 60.12 M\n"
     ]
    }
   ],
   "source": [
    "# load best model\n",
    "model.load_state_dict(torch.load(\"best_mlp.pt\"))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_val = model(X_val_t).cpu().numpy()\n",
    "\n",
    "corr = pearsonr(y_val.flatten(), y_pred_val.flatten())[0]\n",
    "ev   = explained_variance_score(y_val, y_pred_val, multioutput='raw_values')\n",
    "mse  = mean_squared_error(y_val, y_pred_val, multioutput='raw_values')\n",
    "\n",
    "print(\"\\n=== Validation metrics ===\")\n",
    "print(f\"Pearson r (all spikes flattened): {corr:.4f}\")\n",
    "print(f\"Explained variance (mean over neurons): {ev.mean():.4f}\")\n",
    "print(f\"MSE (mean over neurons): {mse.mean():.4f}\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters())/1e6:.2f} M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8be2012",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
